# -*- coding: utf-8 -*-
"""DL(Experiment 1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nEKZGplUwxB-icBGj0NsVCYVUkH5h8J2
"""

### To Read CSV file from Google Drive :

from google.colab import auth
import gspread
from google.auth import default
import pandas as pd

## To train and test 
from sklearn.model_selection import train_test_split

## To scale 
from sklearn.preprocessing import MinMaxScaler

## To create a neural network model
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

### Authenticate User:

auth.authenticate_user()
creds, _ = default()
gc = gspread.authorize(creds)

### Open the Google Sheet and convert into DataFrame :

worksheet = gc.open('sheet_for_DL').sheet1
rows = worksheet.get_all_values()
df = pd.DataFrame(rows[1:], columns = rows[0])

df = df.astype({'Input':'float'})
df = df.astype({'Output':'float'})

df

X = df[['Input']].values
y = df[['Output']].values

X

y

X_train,X_test,y_train,y_test= train_test_split(X,y,test_size = 0.4, random_state =35)

Scaler = MinMaxScaler()

Scaler.fit(X_train)

X_train1 = Scaler.transform(X_train)

ai = Sequential([Dense(5 , activation = 'relu') ,Dense(10,activation = 'relu'), Dense(1)])

ai.compile(optimizer = 'rmsprop' , loss = 'mse')

ai.fit(X_train1 , y_train,epochs = 1900)

loss_df = pd.DataFrame(ai.history.history)

loss_df.plot()

X_test1 =Scaler.transform(X_test)

ai.evaluate(X_test1,y_test)

X_n1=[[4]]

X_n1_1=Scaler.transform(X_n1)

ai.predict(X_n1_1)